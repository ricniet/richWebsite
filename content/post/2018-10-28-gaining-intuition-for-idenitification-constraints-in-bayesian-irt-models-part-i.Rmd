---
title: 'Gaining Intuition for Idenitification Constraints in Bayesian IRT Models:
  Part I'
author: 'Rich '
date: '2018-10-28'
slug: gaining-intuition-for-idenitification-constraints-in-bayesian-irt-models-part-i
categories:
  - Tutorials
  - IRT
tags:
  - IRT
  - Tutorials
  - JAGS
  - R
  - Bayesian
image:
  caption: ''
  focal_point: ''
---

I've spent considerable time developing JAGS/Stan code for fitting Bayesian IRT models. Understanding (let alone resolving) identification constraints has to be one of the most frustrating stages in the workflow. I think this is largely because constraints are already configured in the software we use to implement IRT analyses, so there's just little reason to know the details! But for those of you who've fit latent variable models as Bayesian hierarchical models, you'll know that understanding *why* such constraints are necessary, *what* options are available for model identification, and *what* their implications are for parameter interpretation can be really confusing. 

Even if you haven't had to manually identify an IRT model before, it's worth gaining intuition for it for at least 3 reasons:

1. It's good to understand the defaults that a particular software/package provides, especially when making comparisons;
2. It's really helpful (and more important!) when you move to more complex models, such as explanatory or multidimensional IRT models;
3. Latent variable models are inherently ambiguous, and identifying the metric on which parameters lie is a great exercise in becoming more comfortable with them. 

In the first of (at least) two posts, I'm going to cover identification for a few of the more common IRT models. I'll touch on constraints conceptually, but the real goal is to *show* you different ways of identifying IRT models in JAGS. I'll focus on a few things with each example:

* Why constraints are necessary
* Different options for setting constraints
* Why we might (not) want to use a given set of constraints

Before diving in, it's important to stress that **identification is a property of the likelihood model**, and thus, despite this post focusing on a Bayesian approach, it applies to frequentist methods as well.  

## IRT Models for Binary Responses

### I. Rasch Model

Let's start with the simplest of IRT models. Letting $i$ index individuals, and $j$ index items, the Rasch model specifies the log-odds of a correct respone as the difference between an individual's latent trait $\theta_i$ and item's difficulty $\beta_j$. Let's denote this difference $\eta_{ij}$:

$$ \text{logit}\big\{p\big(y_{ij} = 1 | \Theta\big)\big\} = \theta_i - \beta_j = \eta_{ij}.$$

#### Is this model identified? 

The answer, of course, is no. The issue here is one that plagues all latent variable models---there is no inherent metric when we're measuring unobserved variables, and so identifying a unique solution (i.e., obtaining parameter estimates) is impossible without first imposing a scale via constraints.

The real issue is that predictions from this model (e.g., logits, probability) depend on the *relative* positions of the parameters. The issue is made evident when we consider the difference $\eta_{ij}$. Note that an endless number of solutions for $\theta_i$ and $\beta_j$ result in the same $\eta_{ij}$ (and by extension, likelihood). In the literature, this is often referred to as *additive* aliasing---adding the same constant to both $\theta_i$ and $\beta_j$ results in the same likelihood.

To make this really clear, suppose the "true" value of $\eta_{11}$ is $2$. Well, setting $\theta_1 = 3$ and $\beta_1 = 1$ results in $\eta_{11}=3 - 1 =2$. But setting $\theta_1=102$ and $\beta_1=100$ also leads to $\eta_{11} = 102-100=2$. And for that matter, $\eta_{11} = -3 - -5 = 2$, and $\eta_{11} = 2.3 - 0.3 = 2$, and $\eta_{11} = 3,222 - 3,000 = 2$, and on we go on an endless quest to find a unique solution to an impossible mathematical problem.

#### How do we identify this model?

Because we're interested in the *relative* difference between the latent trait and item difficulty, what we need is an anchoring point that will impose a metric and allow a unique solution to be found. There are *many* options for constraining the model, but I'll focus on those most common in the psychometric literature:

1. Constraining the mean and variance of the latent trait distribution, such that $\theta_i \sim \mathcal{N}\big(0, 1\big)$ for all $i$
2. Constraining the mean of the item difficulties


### II. Two-Parameter Logistic (2PL) Model

$$ \text{logit}\big\{p\big(y_{ij} = 1 | \Theta\big)\big\} = \alpha_j\big(\theta_i - \beta_j\big) = \eta_{ij}.$$

## IRT Models for Multinomial/Ordinal Responses

### Graded Response Model
