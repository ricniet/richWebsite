---
title: 'Gaining Intuition for Idenitification Constraints in Bayesian IRT Models:
  Part I'
author: 'Rich '
date: '2018-10-28'
slug: gaining-intuition-for-idenitification-constraints-in-bayesian-irt-models-part-i
categories:
  - Tutorials
  - IRT
tags:
  - IRT
  - Tutorials
  - JAGS
  - R
  - Bayesian
image:
  caption: ''
  focal_point: ''
---



<p>I’ve spent considerable time developing JAGS/Stan code for fitting Bayesian IRT models. Understanding (let alone resolving) identification constraints has to be one of the most frustrating stages in the workflow. I think this is largely because constraints are already configured in the software we use to implement IRT analyses, so there’s just little reason to know the details! But for those of you who’ve fit latent variable models as Bayesian hierarchical models, you’ll know that understanding <em>why</em> such constraints are necessary, <em>what</em> options are available for model identification, and <em>what</em> their implications are for parameter interpretation can be really confusing.</p>
<p>Even if you haven’t had to manually identify an IRT model before, it’s worth gaining intuition for it for at least 3 reasons:</p>
<ol style="list-style-type: decimal">
<li>It’s good to understand the defaults that a particular software/package provides, especially when making comparisons;</li>
<li>It’s really helpful (and more important!) when you move to more complex models, such as explanatory or multidimensional IRT models;</li>
<li>Latent variable models are inherently ambiguous, and identifying the metric on which parameters lie is a great exercise in becoming more comfortable with them.</li>
</ol>
<p>In the first of (at least) two posts, I’m going to cover identification for a few of the more common IRT models. I’ll touch on constraints conceptually, but the real goal is to <em>show</em> you different ways of identifying IRT models in JAGS. I’ll focus on a few things with each example:</p>
<ul>
<li>Why constraints are necessary</li>
<li>Different options for setting constraints</li>
<li>Why we might (not) want to use a given set of constraints</li>
</ul>
<p>Before diving in, it’s important to stress that <strong>identification is a property of the likelihood model</strong>, and thus, despite this post focusing on a Bayesian approach, it applies to frequentist methods as well.</p>
<div id="irt-models-for-binary-responses" class="section level2">
<h2>IRT Models for Binary Responses</h2>
<div id="i.-rasch-model" class="section level3">
<h3>I. Rasch Model</h3>
<p>Let’s start with the simplest of IRT models. Letting <span class="math inline">\(i\)</span> index individuals, and <span class="math inline">\(j\)</span> index items, the Rasch model specifies the log-odds of a correct respone as the difference between an individual’s latent trait <span class="math inline">\(\theta_i\)</span> and item’s difficulty <span class="math inline">\(\beta_j\)</span>. Let’s denote this difference <span class="math inline">\(\eta_{ij}\)</span>:</p>
<p><span class="math display">\[ \text{logit}\big\{p\big(y_{ij} = 1 | \Theta\big)\big\} = \theta_i - \beta_j = \eta_{ij}.\]</span></p>
<div id="is-this-model-identified" class="section level4">
<h4>Is this model identified?</h4>
<p>The answer, of course, is no. The issue here is one that plagues all latent variable models—there is no inherent metric when we’re measuring unobserved variables, and so identifying a unique solution (i.e., obtaining parameter estimates) is impossible without first imposing a scale via constraints.</p>
<p>The issue is made evident when we consider the difference <span class="math inline">\(\eta_{ij}\)</span>. Note that an endless number of solutions for <span class="math inline">\(\theta_i\)</span> and <span class="math inline">\(\beta_j\)</span> result in the same <span class="math inline">\(\eta_{ij}\)</span> (and by extension, likelihood). In the literature, this is often referred to as <em>additive</em> aliasing—adding the same constant to both <span class="math inline">\(\theta_i\)</span> and <span class="math inline">\(\beta_j\)</span> results in the same likelihood.</p>
<p>To make this really clear, suppose the “true” value of <span class="math inline">\(\eta_{11}\)</span> is <span class="math inline">\(2\)</span>. Well, setting <span class="math inline">\(\theta_1 = 3\)</span> and <span class="math inline">\(\beta_1 = 1\)</span> results in <span class="math inline">\(\eta_{11}=3 - 1 =2\)</span>. But setting <span class="math inline">\(\theta_1=102\)</span> and <span class="math inline">\(\beta_1=100\)</span> also leads to <span class="math inline">\(\eta_{11} = 102-100=2\)</span>. And for that matter, <span class="math inline">\(\eta_{11} = -3 - -5 = 2\)</span>, and <span class="math inline">\(\eta_{11} = 2.3 - 0.3 = 2\)</span>, and <span class="math inline">\(\eta_{11} = 3,222 - 3,000 = 2\)</span>, and on we go on an endless quest to find a unique solution to an impossible mathematical problem.</p>
</div>
<div id="how-do-we-identify-this-model" class="section level4">
<h4>How do we identify this model?</h4>
<p>Because we’re interested in the <em>relative</em> difference between the latent trait and item difficulty, what we need is an anchoring point that will impose a metric and allow a unique solution to be found. There are <em>many</em> options for constraining the model, but I’ll focus on those most common in the psychometric literature:</p>
<ol style="list-style-type: decimal">
<li>Constraining the mean and variance of the latent trait distribution, such that <span class="math inline">\(\theta_i \sim \mathcal{N}\big(0, 1\big)\)</span> for all <span class="math inline">\(i\)</span></li>
<li>Constraining the mean of the item difficulties</li>
</ol>
</div>
</div>
<div id="ii.-two-parameter-logistic-2pl-model" class="section level3">
<h3>II. Two-Parameter Logistic (2PL) Model</h3>
<p><span class="math display">\[ \text{logit}\big\{p\big(y_{ij} = 1 | \Theta\big)\big\} = \alpha_j\big(\theta_i - \beta_j\big) = \eta_{ij}.\]</span></p>
</div>
</div>
<div id="irt-models-for-multinomialordinal-responses" class="section level2">
<h2>IRT Models for Multinomial/Ordinal Responses</h2>
<div id="graded-response-model" class="section level3">
<h3>Graded Response Model</h3>
</div>
</div>
